<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>L1 Sample Flow for Efficient Visuomotor Learning</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Preconnect for shared assets -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">

  <!-- Favicon
  <link rel="icon" type="image/x-icon" href="./static/images/favicon.ico"> -->

  <!-- Critical CSS -->
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <!-- Non-critical CSS -->
  <link rel="preload" href="./static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="./static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="./static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">

  <noscript>
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>

  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">

  <!-- Shared JS -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script defer src="./static/js/bulma-carousel.min.js"></script>
  <script defer src="./static/js/bulma-slider.min.js"></script>
  <script defer src="./static/js/index.js"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <style>
    .publication-links { margin-top: 20px; }
    .publication-authors {
      display: flex;
      flex-wrap: wrap;
      justify-content: center;
      gap: 0.4rem 1rem;
      line-height: 1.4;
      margin-top: 16px;
      text-align: center;
    }
    .publication-authors .author-block {
      margin: 0;
      font-family: 'Inter', sans-serif;
    }
    .publication-authors.affiliations {
      display: block;
      margin-top: 10px;
      line-height: 1.5;
    }
    .abstract-box { background-color: #f5f5f5; padding: 30px; border-radius: 10px; }
    .figure-caption { font-size: 0.9rem; color: #666; margin-top: 10px; }
    .teaser-video { width: 100%; border-radius: 16px; box-shadow: 0 10px 20px rgba(0,0,0,0.1); }
    .synced-video { max-height: 360px; }
    .image-container { text-align: center; margin-bottom: 20px; }
    img { max-width: 100%; height: auto; border-radius: 5px; }
    #bibtex-code {
      white-space: pre-wrap;
      word-break: break-word;
      overflow-x: visible;
    }
  </style>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          
          <h1 class="title is-1 publication-title">L1 Sample Flow for Efficient Visuomotor Learning
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="www.scholar.com">Weixi Song</a><sup>1,2,3</sup>,</span>
            <span class="author-block">
              <a href="#">Zhetao Chen</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="#">Tao Xu</a><sup>2</sup>
            </span>
            <span class="author-block">
                <a href="#">Xianchao Zeng</a><sup>2</sup>
            </span>
            <span class="author-block">
                <a href="#">Xinyu Zhou</a><sup>2</sup>
            </span>
            <span class="author-block">
                <a href="#">Lixin Yang</a><sup>2,4</sup>
            </span>
            <br>
            <span class="author-block">
                <a href="#">Donglin Wang</a><sup>3&dagger;</sup>
            </span>
            <span class="author-block">
                <a href="#">Cewu Lu</a><sup>2,4</sup>
            </span>
            <span class="author-block">
                <a href="#">Yonglu Li</a><sup>2,4&dagger;</sup>
            </span>
          </div>
          <span class="author-block" style="font-size:medium;"><sup>&dagger;</sup>Corresponding Author</span>
          <div class="is-size-5 publication-authors affiliations">
            <span class="author-block"><sup>1</sup>Zhejiang University,</span>
            <span class="author-block"><sup>2</sup>Shanghai Innovation Institute</span>
            <span class="author-block"><sup>3</sup>Westlake University</span>
            <br>
            <span class="author-block"><sup>4</sup>Shanghai Jiao Tong University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="./static/pdfs/L1_Sample_Flow__Arxiv_Vers__.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2511.17898" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/song-wx/L1Flow.github.io" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span>Code</span>
                </a>
              </span>
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<div class="image-container">
  <img src="./static/images/outline.jpg" alt="Method Diagram" style="width: 40%;">
  <p class="figure-caption">Overview.</p>
</div>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
      </div>
    </div>
    <div class="content has-text-justified abstract-box">
      <p>
        Denoising-based models, such as diffusion and flow matching, have been a critical component of robotic manipulation for their strong distribution-fitting and scaling capacity. Concurrently, several works have demonstrated that simple learning objectives, such as L1 regression, can achieve performance comparable to denoising-based methods on certain tasks, while offering faster convergence and inference. In this paper, we focus on how to combine the advantages of these two paradigms: <strong>retaining the ability of denoising models to capture multi-modal distributions and avoid mode collapse while achieving the efficiency of the L1 regression objective.</strong> 
      </p>
      <p>
        To achieve this vision, we reformulate the original v-prediction flow matching and transform it into sample-prediction with the L1 training objective. We empirically show that the multi-modality can be expressed via a single ODE step. Thus, we propose <strong>L1 Flow</strong>, a two-step sampling schedule that generates a suboptimal action sequence via a single integration step and then reconstructs the precise action sequence through a single prediction. The proposed method largely retains the advantages of flow matching while reducing the iterative neural function evaluations to merely two and mitigating the potential performance degradation associated with direct sample regression. 
      </p>
      <p>
        We evaluate our method with varying baselines and benchmarks, including 8 tasks in MimicGen, 5 tasks in RoboMimic   & PushT Bench, and one task in the real-world scenario. The results show the advantages of the proposed method with regard to training efficiency, inference speed, and overall performance.
      </p>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
      </div>
    </div>
    <p>
        We consider a variant of Flow Matching where the model directly predicts the terminal sample rather than the instantaneous velocity. Instead of learning the instantaneous velocity field \(x_1-x_0\), the model \(\textcolor{red}{f_\theta(x_t,t)}\) predicts the corresponding terminal sample \(x_1\) conditioned on the intermediate state \(x_t\) and time \(t\):
        \begin{aligned}
          \text{Origin:} &\quad \textcolor{red}{f_\theta(x_t, t)} = \frac{dx_t}{dt} = x_1 - x_0 \\
          \text{Now:}    &\quad \textcolor{red}{f_\theta(x_t, t)} = x_1
        \end{aligned}
        The instantaneous velocity can then be implicitly recovered as: 
        \begin{equation}
        \begin{aligned}
            v=\frac{dx}{dt}=x_1-x_0&=\frac{\textcolor{red}{f_\theta(x_t,t)}-x_t}{1-t}.  
        \end{aligned}
        \end{equation}
        At training time, the model is optimized to minimize the discrepancy between the predicted terminal sample \(x_{pred}\) and the true target sample \(x_1\). We employ L1 loss to supervise the target samples:
        \begin{equation}
        \mathcal{L}=\mathbb{E}_{x_0\sim\mathcal{N}(0,1),x_1\sim data,t}\ ||\textcolor{red}{f_\theta(x_t,t)}-x_1||_1.
        \label{eq:x_loss_L1}
        \end{equation}
        During inference, we introduce a two-step denoising schedule that combines continuous flow integration with direct sample prediction. We first integrate the learned flow field from \( t=0 \) to an intermediate time \( t=0.5 \):
        \begin{equation}
            x_{0.5}=x_0+\frac{\textcolor{red}{f_\theta(x_0,0)}-x_0}{2}.
        \end{equation}
        Then at the midpoint, we directly invoke the model's sample prediction capability to obtain the final output:
        \begin{equation}
            x_{1} = \textcolor{red}{f_\theta(x_{0.5}, 0.5)}.
        \end{equation}
        See details in the pseudocode.
      </p>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Pseudocode</h2>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-three-quarters">
        <div class="content has-text-justified">
          <div class="image-container">
            <img src="./static/images/algo.jpg" alt="Method pseudocode" style="width: 100%; max-width: 600pt; display:inline-block;">
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experimental Results</h2>
      </div>
    </div>
    <div class="has-text-justified">
      <p style="margin-bottom: 1.2rem;">
        We first compare the proposed two-step sampling strategy with 2 standard denoising-based methods ie. DDPM (100 steps) and Flow Matching (10 steps), and L1 regression in 8 tasks of MimicGen Benchmark.
      </p>
      <div class="image-container">
         <img src="./static/images/exp1.jpg" alt="Results">
      </div>
      <p style="margin: 1.5rem 0;">
        Then, as an efficient paradigm aiming at speeding up, L1 Flow is compared to distillation-based methods ie. Consistency Policy (CP) and OneDP in total 5 tasks of Robomimic and PushT Bench.
      </p>
      <div class="image-container">
        <img src="./static/images/exp2.jpg" alt="Results">
      </div>
    </div>
    <figure class="has-text-centered">
      <div class="columns is-centered">
        <div class="column is-full">
          <div class="content has-text-centered">
            <video class="teaser-video" muted controls playsinline loop preload="metadata">
              <source src="./static/videos/ALL.mp4" type="video/mp4">
            </video>
            <h2 class="subtitle has-text-centered" style="font-size: 15pt;">Real-world Experiments </h2>
          </div>
        </div>
      </div>
    </figure>
  </div>
</section>

<div class="image-container">
  <img src="./static/images/exp3.jpg" alt="Overall quantitative comparison" style="width: 100%; max-width: 600pt;">
  <p class="figure-caption">Comparison on Real-world Experiments.</p>
</div>

<section class="section" id="bibtex">
  <div class="container is-max-desktop content">
    <div class="bibtex-header">
      <h2 class="title is-3">BibTeX</h2>
      <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
        <i class="fas fa-copy"></i>
        <span class="copy-text">Copy</span>
      </button>
    </div>
    <pre id="bibtex-code"><code>@misc{song2025l1sampleflowefficient,
  title={L1 Sample Flow for Efficient Visuomotor Learning}, 
  author={Weixi Song and Zhetao Chen and Tao Xu and Xianchao Zeng and Xinyu Zhou and Lixin Yang and Donglin Wang and Cewu Lu and Yong-Lu Li},
  year={2025},
  eprint={2511.17898},
  archivePrefix={arXiv},
  primaryClass={cs.RO},
  url={https://arxiv.org/abs/2511.17898}, 
}</code></pre>
  </div>
</section>

</body>
</html>
